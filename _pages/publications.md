---
layout: single
title: ""
permalink: /publications/
author_profile: true
---

* **Do Pre-trained Transformers Really Learn In-context by Gradient Descent?**  
Lingfeng Shen, Aayush Mishra and Daniel Khashabi.  
arXiv preprint [arXiv:2310.08540](https://arxiv.org/abs/2310.08540), 2023.

* **SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation.**  
Abe Bohan Hou and Jingyu Zhang and Tianxing He and Yichen Wang and Yung-Sung Chuang and Hongwei Wang and Lingfeng Shen and Benjamin Van Durme and Daniel Khashabi and Yulia Tsvetkov.  
arXiv preprint [arXiv:2310.03991](https://arxiv.org/abs/2310.03991), 2023.

* **Error Norm Truncation: Robust Training in the Presence of Data Noise for Text Generation Models.**  
Tianjian Li and Haoran Xu and Philipp Koehn and Daniel Khashabi and Kenton Murray.  
arXiv preprint [arXiv:2310.00840](https://arxiv.org/abs/2310.00840), 2023. [\[code\]](https://link-to-code)
