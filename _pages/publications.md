---
layout: single
title: ""
permalink: /publications/
author_profile: true
---

* **Do Pre-trained Transformers Really Learn In-context by Gradient Descent?**  
Lingfeng Shen, Aayush Mishra and Daniel Khashabi.  
arXiv preprint [arXiv:2310.08540](https://arxiv.org/abs/2310.08540), 2023.


* **Speech simclr: Combining contrastive and reconstruction objective for self-supervised speech representation learning**  
Dongwei Jiang, Wubo Li, Miao Cao, Wei Zou, Xiangang Li

InterSpeech, 2021.

* **Transformer based unsupervised pre-training for acoustic representation learning** 
Ruixiong Zhang, Haiwei Wu, Wubo Li, Dongwei Jiang, Wei Zou, Xiangang Li

ICASSP, 2021.

* **Didispeech: A large scale mandarin speech corpus**
Tingwei Guo, Cheng Wen, Dongwei Jiang, Ne Luo, Ruixiong Zhang, Shuaijiang Zhao, Wubo Li, Cheng Gong, Wei Zou, Kun Han, Xiangang Li

ICASSP, 2021.

* **A further study of unsupervised pretraining for transformer based speech recognition**  
Dongwei Jiang, Wubo Li, Ruixiong Zhang, Miao Cao, Ne Luo, Yang Han, Wei Zou, Kun Han, Xiangang Li
ICASSP, 2021.

* **TMT: A transformer-based modal translator for improving multimodal sequence representations in audio visual scene-aware dialog**
Wubo Li, Dongwei Jiang, Wei Zou, Xiangang Li
InterSpeech, 2020

* **Improving transformer-based speech recognition using unsupervised pre-training**  
Dongwei Jiang, Xiaoning Lei, Wubo Li, Ne Luo, Yuxuan Hu, Wei Zou, Xiangang Li
arXiv preprint [arXiv:1910.09932](https://arxiv.org/pdf/1910.09932.pdf), 2019.

* **Towards end-to-end code-switching speech recognition**  
Ne Luo, Dongwei Jiang, Shuaijiang Zhao, Caixia Gong, Wei Zou, Xiangang Li
arXiv preprint [arXiv:1810.13091](https://arxiv.org/abs/1810.13091), 2018.

* **Comparable study of modeling units for end-to-end mandarin speech recognition** 
Wei Zou, Dongwei Jiang, Shuaijiang Zhao, Guilin Yang, Xiangang Li
ISCSLP, 2018.

* **An Analysis of Decoding for Attention-Based End-to-End Mandarin Speech Recognition**
Dongwei Jiang, Wei Zou, Shuaijiang Zhao, Guilin Yang, Xiangang Li
ISCSLP, 2018



